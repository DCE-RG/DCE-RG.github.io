---
layout: default
title: Home
isHome: true
---

<section class="bs-docs-section">
  <h1 id="overview" class="page-header">Overview</h1>

  <blockquote>
    This is a weakly reading group of Data-Centric Engineering programme in The Alan Turing Institute (<a target="_blank" rel="noopener noreferrer" href="https://www.turing.ac.uk/research/research-programmes/data-centric-engineering" style="color:gray;">https://www.turing.ac.uk/research/research-programmes/data-centric-engineering</a>).  
    The reading / talk topics are broad on statistical theory, methodology, and application e.g. stocastic process, optimization, surrogate modelling, and etc.  
    Please see our schedule below to check upcoming talks.  
    This group is open to everyone and there is no requirement in order to join each talks.  
    If you would like to register for our emailing list or to give your talk in our reading group, please feel free to contact organizers.  
  </blockquote>

</section>


<section class="bs-docs-section">
  <h1 id="schedule" class="page-header">Schedule</h1>

  <h2 id="january">January (Invited Talks + Surrogate Modelling)</h2>
  <blockquote>
    <h3>15th Wednesday 11:00 - 12:00 @ Lovelace Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Monte Carlo wavelets: a randomized approach to frame discretization </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Prof Lorenzo Rosasco (University of Genoa, Italy) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0115" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse in" id="abstract_0115">
        <div class="card card-body abstract-text">
            In this paper we propose and study a family of continuous wavelets on general domains, and a corresponding stochastic discretization that we call Monte Carlo wavelets. 
            First, using tools from the theory of reproducing kernel Hilbert spaces and associated integral operators, we define a family of continuous wavelets by spectral calculus. 
            Then, we propose a stochastic discretization based on Monte Carlo estimates of integral operators. 
            Using concentration of measure results, we establish the convergence of such a discretization and derive convergence rates under natural regularity assumptions.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>22nd Wednesday 11:00 - 12:00 @ Margaret Hamilton Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Bayesian Optimal Design for iterative refocussing </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dr Victoria Volodina (The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0122" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse in" id="abstract_0122">
        <div class="card card-body abstract-text">
            History matching is a type of calibration, that attempts to find input parameters values to achieve the consistency between observations and computer model representation.
            History matching is most effective when it is performed in waves, i.e. refocussing steps (Williamson et al., 2017). 
            At each wave a new ensemble is obtained within the current Not Ruled Out Yet space (NROY), the emulator is updated and the procedure of cutting down the input space is performed again. </br> 
            </br> 
            Generating design for each wave is a challenging problem due to the unusual shapes of NROY space. 
            A number of approaches (Williamson and Vernon, 2013; Gong et al., 2016, Andrianakis et al., 2017) are focused on obtaining space-filling design over the NROY space. 
            In this talk we present a new decision-theoretic method for a design problem for iterative refocussing. 
            We employ a Bayesian experimental design and specify a loss function that compares a volume of NROY space obtained with an updated emulator to the volume of `true' NROY space obtained using a `perfect' emulator.  
            The derived expected loss function contains three independent and interpretable terms. 
            In this talk we compare the effect of proposed Bayesian Optimal Design to space-filling design approaches on the iterative refocussing performed on simulation studies. </br>
            </br> 
            We recognise that adopted Bayesian experimental design involves an expensive optimization problem. 
            Our proposed criterion also could be used to investigate and rank a range of candidate designs for iterative refocussing. 
            In this talk we demonstrate the mathematical justification provided by our Bayesian Design Criterion for each design candidate.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>29th Wednesday 11:00 - 12:00 @ Margaret Hamilton Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Integrated Emulators for Systems of Computer Models </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Mr Deyu Ming (University College London, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0129" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse in" id="abstract_0129">
        <div class="card card-body abstract-text">
            We generalize the state-of-the-art linked emulator for a system of two computer models under the squared exponential kernel to an integrated emulator for any feed-forward system of multiple computer models, under a variety of kernels (exponential, squared exponential, and two key Mat√©rn kernels) that are essential in advanced applications. 
            The integrated emulator combines Gaussian process emulators of individual computer models, and predicts the global output of the system using a Gaussian distribution with explicit mean and variance. 
            By learning the system structure, our integrated emulator outperforms the composite emulator, which emulates the entire system using only global inputs and outputs. 
            Orders of magnitude prediction improvement can be achieved for moderate-size designs. 
            Furthermore, our analytic expressions allow a fast and efficient design algorithm that allocates different runs to individual computer models based on their heterogeneous functional complexity. 
            This design yields either significant computational gains or orders of magnitude reductions in prediction errors for moderate training sizes. We demonstrate the skills and benefits of the integrated emulator in a series of synthetic experiments and a feed-back coupled fire-detection satellite model.
        </div>
    </div>
  </blockquote>

  <h2 id="february">February (Invited Talks + Surrogate Modelling)</h2>
  <blockquote>
    <h3>5th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: A kernel log-rank test of independence</h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dr Tamara Fernadez (University College London, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0205" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse in" id="abstract_0205">
        <div class="card card-body abstract-text">
            With the incorporation of new data gathering methods in clinical research, it becomes fundamental for Survival Analysis techniques to be able to deal with high-dimensional or/and non-standard covariates.
            In this paper we introduce a general non-parametric independence test between right-censored survival times and covariates taking values on a general space $\mathcal{X}$. 
            We show our test statistic has a dual intepretation, first in terms of the supremum of a potentially infinite collection of weight-indexed log-rank tests, with weight functions belonging to a Reproducing kernel Hilbert space of functions (RKHS), and second, as the norm of the difference of the embeddings of certain ``depencency" measures into the RKHS, similarly to the well-know HSIC test-statistic. 
            We provide an easy-to-use test-statistic as well as an economic Wild-Bootstrap procedure and study asymptotic properties of the test, finding sufficient conditions to ensure our test is omnibus. 
            We perform extensive simulations demonstrating that our testing procedure performs, in general, better than competing approaches.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>12nd Wednesday 11:00 - 12:00 @ Lovelace Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Surrogate modelling (TBA)</h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dr James Salter (University of Exeter, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0212" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse" id="abstract_0212">
        <div class="card card-body abstract-text">
            TBA
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>19th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Physical Modelling (TBA) </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dr Monika Kreitmair (University of Cambridge / The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0219" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse" id="abstract_0219">
        <div class="card card-body abstract-text">
            TBA
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>26th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: On the geometry of Stein variational gradient descent</h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dr Andrew Duncan (Imperial College London / The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0226" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse in" id="abstract_0226">
        <div class="card card-body abstract-text">
                Bayesian inference problems require sampling or approximating high-dimensional probability distributions. 
                The focus of this paper is on the recently introduced Stein variational gradient descent methodology, a class of algorithms that rely on iterated steepest descent steps with respect to a reproducing kernel Hilbert space norm. 
                This construction leads to interacting particle systems, the mean-field limit of which is a gradient flow on the space of probability distributions equipped with a certain geometrical structure. 
                We leverage this viewpoint to shed some light on the convergence properties of the algorithm, in particular addressing the problem of choosing a suitable positive definite kernel function. 
                Our analysis leads us to considering certain nondifferentiable kernels with adjusted tails. 
                We demonstrate significant performs gains of these in various numerical experiments.
        </div>
    </div>
  </blockquote>

  <h2 id="march">March</h2>
  <blockquote>
    <h3>6th Friday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: TBA </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dr Helen Ogden (University of Southampton / The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0306" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse" id="abstract_0306">
        <div class="card card-body abstract-text">
            TBA
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>Upcoming more!</h3>
  </blockquote>

  <h2 id="april-december">April ~ December (under arrangement)</h2>

</section>

<section class="bs-docs-section">
    <h1 id="past_schedule_2019" class="page-header">Past Schedule (2019) <a data-toggle="collapse" href="#div_past_schedule_2019" role="button" style="color:gray;"> [click here!] </a></h1>
    <div class="collapse" id="div_past_schedule_2019">
      
      <h2>Optimization</h2>
      <blockquote>
          <h3>6th February 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Stochastic Gradient Descent </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Marina Riabiz </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
                <li>Leon Bottou, Frank E. Curtis, Jorge Nocedal, <i>Optimization Methods for Large-Scale Machine Learning,</i> <a href="https://arxiv.org/pdf/1606.04838.pdf#page21">https://arxiv.org/pdf/1606.04838.pdf#page21</a></li>
            </ul>
          </div>
      </blockquote>
  
      <blockquote>
          <h3>13th February 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Proof of convergence rate of Stochastic Gradient Descent </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: √ñmer Deniz Akyƒ±ldƒ±z </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
          </div>
      </blockquote>
  
      <blockquote>
          <h3>20th February 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Proof of convergence rate of Stochastic Gradient Descent </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: √ñmer Deniz Akyƒ±ldƒ±z </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
                <li>Robert M. Gower, <i>Convergence Theorems for Gradient Descent, </i><a href="https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf">https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf</a></li>
                <li>Ji Liu, <i>Stochastic Gradient ‚ÄòDescent‚Äô Algorithm</i>, https://www.cs.rochester.edu/u/jliu/CSC-576/class-note-10.pdf </li>
            </ul>
          </div>
      </blockquote>
  
      <blockquote>
          <h3>27th February 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Stochastic Gradient Langevin Dynamics </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Andrew Duncan </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
                <li>Maxim Raginsky, Alexander Rakhlin, Matus Telgarsky, <i>Non-Convex Learning via Stochastic Gradient Langevin Dynamics: A Nonasymptotic Analysis,</i> https://arxiv.org/pdf/1702.03849.pdf </li>
            </ul>
          </div>
      </blockquote>
  
      <blockquote>
          <h3>6th March 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Conjugate Gradient Methods </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Taha Ceriti </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
              <li>Chris Bishop, <i>Neural Networks for Pattern Recognition</i>, Chapter 7. </li>
              <li>J.R. Shewchuk, <i>An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</i>, 1994, https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf </li>
            </ul>
          </div>
      </blockquote>
  
      <blockquote>
          <h3>(Extra Reference)</h3>
          <div class="card card-body abstract-text">
            <ul type="square">
              <li>S. Bubeck, Convex Optimization: Algorithms and Complexity. In Foundations and Trends in Machine Learning, Vol. 8: No. 3-4, pp 231-357, 2015. <a href="http://sbubeck.com/Bubeck15.pdf" rel="nofollow">http://sbubeck.com/Bubeck15.pdf</a> </li>
              <li>Nagapetyan et al., The True Cost of SGLD, https://arxiv.org/pdf/1706.02692.pdf </li>
              <li>Brosse at al, The promises and pitfalls of Stochastic Gradient Langevin Dynamics, https://arxiv.org/pdf/1811.10072.pdf </li>
              <li>Dalalyan and Karagulyan, User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient, https://arxiv.org/pdf/1710.00095.pdf </li>
              <li>Vollmer et al., Exploration of the (Non-)asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics, https://arxiv.org/pdf/1501.00438.pdf </li>
            </ul>
          </div>
      </blockquote>
  
      <h2>Gaussian Processes and RKHS</h2>
  
      <blockquote>
          <h3>13th March 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Hyperparameter estimation for Gaussian Processes </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Alex Diaz </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
                <li>Rassmussen, C.E., Gaussian Processes for Machine Learning (Ch2 and Ch 5) <u><a href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf" rel="nofollow">http://www.gaussianprocess.org/gpml/chapters/RW.pdf</a></u></li>
                <li>DiazDelaO, F.A. et al. (2017) Bayesian updating and model class selection with Subset Simulation <u> https://www.sciencedirect.com/science/article/pii/S0045782516308283</u></li>
                <li>Garbuno-Inigo, A. et al. (2016) Gaussian process hyper-parameter estimation using Parallel Asymptotically Independent Markov Sampling <u>https://www.sciencedirect.com/science/article/pii/S0167947316301311</u></li>
                <li>Garbuno-Inigo, A. et al. (2016) Transitional annealed adaptive slice sampling for Gaussian process hyper-parameter estimation <a http://www.dl.begellhouse.com/journals/52034eb04b657aea,55c0c92f02169163,2f3449c01b48b322.html" rel="nofollow">http://www.dl.begellhouse.com/journals/52034eb04b657aea,55c0c92f02169163,2f3449c01b48b322.html</a></li>
            </ul>
          </div>
      </blockquote>

      <blockquote>
        <h3>20th March 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Gaussian Interpolation/Regression Error Bounds </h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: George Wynne </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>Holger Wendland, Christien Rieger, Approximate Interpolation with Applications to Selecting Smoothing Parameters <u>https://link.springer.com/article/10.1007/s00211-005-0637-y</u></li>
          </ul>
        </div>
      </blockquote>
      
      <blockquote>
        <h3>27th March 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Structure of the Gaussian RKHS </h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Toni Karvonen </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>Ha Quang Minh, <i>Some Properties of Gaussian Reproducing Kernel Hilbert Spaces and Their Implications for Function Approximation and Learning Theory, </i><u>https://link.springer.com/article/10.1007/s00365-009-9080-0</u></li>
          </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>(Extra reference)</h3>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>Steinwart, I., Hush, D., & Scovel, C. (2006). An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels. IEEE Transactions on Information Theory, 52(10), 4635‚Äì4643</li>
          </ul>
        </div>
      </blockquote>

      <h2>Invited Talks + Deep GPs</h2>

      <blockquote>
        <h3>03th April 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Bayesian synthetic likelihood </h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Leah South </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>L. F. Price, C. C. Drovandi, A. Lee & D. J. Nott, <i>Bayesian Synthetic Likelihood, </i><u>https://www.tandfonline.com/doi/abs/10.1080/10618600.2017.1302882</u> </li>
          </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>12th April 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Deep Gaussian Processes </h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Kangrui Wang </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>Damianou A, Lawrence N. Deep Gaussian processes <u><a href="http://proceedings.mlr.press/v31/damianou13a.pdf" rel="nofollow">http://proceedings.mlr.press/v31/damianou13a.pdf</a></u></li>
            <li>Dunlop M M, Girolami M A, Stuart A M, et al. How deep are deep Gaussian processes? <u><a href="http://www.jmlr.org/papers/volume19/18-015/18-015.pdf" rel="nofollow">http://www.jmlr.org/papers/volume19/18-015/18-015.pdf</a></u></li>
            <li>Bauer M, van der Wilk M, Rasmussen C E. Understanding probabilistic sparse Gaussian process approximations</i><u>https://arxiv.org/abs/1606.04820</u> </li>
          </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>17th April 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Multi Level Monte Carlo</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Alastair Gregory </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li><a href="http://people.maths.ox.ac.uk/gilesm/files/cgst.pdf" rel="nofollow">http://people.maths.ox.ac.uk/gilesm/files/cgst.pdf</a></li>
            <li>https://people.maths.ox.ac.uk/gilesm/files/OPRE_2008.pdf</li>
          </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>24th April 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Adaptive Bayesian Quadrature</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Matthew Fisher </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <h2>MMD & Stein's Method</h2>

      <blockquote>
        <h3>01st May 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Controlling Convergence with Maximum Mean Discrepancy</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Chris Oates </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Gretton, A., Borgwardt, K., Rasch, M., Sch√∂lkopf, B. and Smola, A.J., 2007. <i>A kernel method for the two-sample-problem</i>. <u><a href="http://papers.nips.cc/paper/3110-a-kernel-method-for-the-two-sample-problem.pdf" rel="nofollow">http://papers.nips.cc/paper/3110-a-kernel-method-for-the-two-sample-problem.pdf</a></u> </li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>08th May 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Introduction to Stein‚Äôs method</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: FX Briol </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            
                <li>Talk will cover Chapter 2 of ‚ÄúChen, L. H. Y., Goldstein, L., & Shao, Q.-M. (2011). Normal Approximation by Stein‚Äôs Method. Springer.‚Äù</li>
                <li>Gorham,  J., Duncan, A., Mackey, L., & Vollmer, S. (2016). Measuring Sample Quality with Diffusions. ArXiv:1506.03039.</li>
                <li>Gorham, J., & Mackey, L. (2017). Measuring Sample Quality with Kernels. In Proceedings of the International Conference on Machine Learning (pp. 1292‚Äì1301).</li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>15th May 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Bochner's Theorem and Maximum Mean Discrepancy</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: George Wynne </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Theorem 9 in <u><a href="http://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf" rel="nofollow">http://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf</a></u> is the result of Bochner's theorem being used in MMD</li>
                <li><u><a href="http://www.math.nus.edu.sg/~matsr/ProbI/Lecture7.pdf" rel="nofollow">http://www.math.nus.edu.sg/~matsr/ProbI/Lecture7.pdf</a></u><b> </b>is a source for proof of Bochner's theorem</li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>(Extra Reference)</h3>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>https://sites.google.com/site/steinsmethod/home</li>
            </ul>
        </div>
      </blockquote>

      <h2>Greedy Algorithms & Uncertainty Exploration in Aerospace</h2>

      <blockquote>
        <h3>19th June 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Convergence Guarantees for Adaptive Bayesian Quadrature Methods</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Motonobu Kanagawa </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Kanagawa, M. and Hennig, P, 2019. <i>Convergence Guarantees for Adaptive Bayesian Quadrature Methods</i>. <u>https://arxiv.org/abs/1905.10271</u> </li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>26th June 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Using machine learning to predict and understand turbulence modelling uncertainties</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Ashley Scillitoe </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <blockquote>
        <h3>03rd July 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Polynomial approximations in uncertainty quantification</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Pranay Seshadri </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Talk will cover Xiu D (2010) ‚ÄúNumerical methods for stochastic computations: a spectral method approach‚Äù. Princeton University Press.</li>
            </ul>
        </div>
      </blockquote>

      <h2>Bayesian Learning and Algorithms</h2>

      <blockquote>
        <h3>10th July 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: A Kernel Stein Test for Comparing Latent Variable Models</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Heishiro Kanagawa </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Kanagawa, H., Jitkrittum, W., Mackey, L., Fukumizu, K., Gretton, A. (2019) <i>A Kernel Stein Test for Comparing Latent Variable Models. </i> <u>https://arxiv.org/abs/1907.00586</u> </li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>17th July 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Multi-resolution Multi-task Gaussian Processes</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Oliver Hamelijnck </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Hamelijnck, O., Damoulas, T., Wang, K., Girolami, M. (2019) <i>Multi-resolution Multi-task Gaussian Processes. </i> <u>https://arxiv.org/pdf/1906.08344.pdf</u> </li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>24th July 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: A Primer on PAC Bayesian Learning</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Benjamin Guedj </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Talk will cover <a href="https://bguedj.github.io/icml2019/index.html">this tutorial from ICML 2019</a></li>
            </ul>
        </div>
      </blockquote>

      <h2>Gradient Flows</h2>

      <blockquote>
        <h3>31th July 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: An Introduction to Measure Transport</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Chris Oates </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <blockquote>
        <h3>06th August 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Gradient Flows for Statistical Computation</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Marina Riabiz </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li> Daneri and Savare', Lecture Notes on Gradient Flows and Optimal Transport, <u>https://arxiv.org/abs/1009.3737</u></li>
                <li> N. Garcia Trillos, D. Sanz-Alonso, The Bayesian update: variational formulations and gradient flows, <u>https://arxiv.org/abs/1705.07382</u></li>
                <li> G.Peyre' and M. Cuturi, Computational Optimal Transport, Chapter 9.3 <u>https://arxiv.org/pdf/1803.00567.pdf</u></li>
                <li> Carrillo, Craig, Patacchini, A blob method for diffusion, <u>https://arxiv.org/abs/1709.09195</u></li>
                <li> Sides <u>  http://web.math.ucsb.edu/~kcraig/math/curriculum_vitae_files/NIPS_120917.pdf</u></li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>14th August 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: The Mathematics of Gradient Flows</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Andrew Duncan </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <blockquote>
        <h3>14th August 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Displacement Convexity and Implications for Variational Inference</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Andrew Duncan </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <h2>Estimators for Intractable Models</h2>

      <blockquote>
        <h3>23th August 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Comparing spatial models in the presence of spatial smoothing</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Earl Duncan</h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <blockquote>
        <h3>28th August 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Fisher efficient inference of intractable models</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Song Liu </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Liu, S., Kanamori, T., Jitkrittum, W., & Chen, Y. (2018). <i>Fisher efficient inference of intractable models. </i> <u>https://arxiv.org/abs/1805.07454</u> </li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>04th September 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Statistical Inference for Generative Models with Maximum Mean Discrepancy</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: FX Briol </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Briol, F-X, Barp, A., Duncan, A. B., Girolami, M. (2019) <i>Statistical inference for generative models with maximum mean discrepancy. </i> <u>https://arxiv.org/abs/1906.05944</u> </li>
            </ul>
        </div>
      </blockquote>

      <h2>Uncertainty Quantification and Probabilistic Numerics</h2>

      <blockquote>
        <h3>11th September 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: A New Approach to Probabilistic Rounding Error Analysis</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Jon Cockayne </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Talk will cover Higham, N., Mary, T. (2018) <i>A new approach to probabilistic rounding analysis. </i> <u>http://eprints.maths.manchester.ac.uk/2673/1/paper.pdf</u> </li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>25th September 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Model Inference for Ordinary Differential Equations by Parametric Polynomial Kernel Regression</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: David Green </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <blockquote>
        <h3>02nd October 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: The Ridgelet Transform and a Quadrature of Neural Networks</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Takuo Matsubara </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <blockquote>
        <h3>09th October 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Hierarchical and multivariate Gaussian processes for environmental and ecological applications</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Jarno Vanhatalo </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <h2>PAC Bayes</h2>

      <blockquote>
        <h3>16th October 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: TBA</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Omar Rivasplata </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <h2>Inference for Stochastic Processes</h2>

      <blockquote>
        <h3>23th October 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Learning Laws of Stochastic Processes</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Harald Oberhauser </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Chevyrev, I., & Oberhauser, H. (2018). Signature moments to characterize laws of stochastic processes. arXiv:1810.10971.</li>
            </ul>
        </div>
      </blockquote>

      <h2>Probabilistic Numerics 2019 Workshop</h2>

      <blockquote>
        <h3>29th October 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>10:00 - 10:15. Chris Oates. Welcome, and Some Open Problems in Probabilistic Numerics.</li>
                <li>10:15 - 10:30. Francois-Xavier Briol. Probabilistic Numerics via Transfer Learning.</li>
                <li>10:30 - 10:45. Toni Karvonen. Uncertainty quantification with Gaussian processes and when to trust a PN method.</li>
                <li>10:45 - 11:00. Simo S√§rkk√§. Numerical Integration as a Finite Matrix Approximation to Multiplication Operator.</li>
                <li>11:30 - 11:45. Motonobu Kanagawa. Open questions regarding adaptive quadrature methods.</li>
                <li>11:45 - 12:00. George Wynne. Gaussian process error bounds from a sampling inequality.</li>
                <li>14:00 - 14:15. Peter Hristov. Surrogate modelling with probabilistic numerics.</li>
                <li>14:15 - 14:30. Filip Tronarp. On Gaussian Filtering/Smoothing for Solving ODEs.</li>
                <li>14:30 - 14:45. Takuo Matsubara. Bayesian quadrature of neural networks based on the Ridgelet transform.</li>
                <li>15:30 - 15:45. Alex Diaz. PN for eigenvalue problems.</li>
                <li>15:45 - 16:00. Maren Mahsereci. Software for PN.</li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>30th October 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>10:00 - 10:15. Alex Gessner. Acquisition functions for adaptive Bayesian quadrature.</li>
                <li>10:15 - 10:30. Mark Girolami. Title TBC.</li>
                <li>10:30 - 10:45. Takeru Matsuda. ODE parameter estimation with discretization error quantification.</li>
                <li>11:30 - 11:45. Matthew Fisher. Locally Adaptive Bayesian Cubature.</li>
                <li>11:45 - 12:00. Daniel Tait. Constrained VI for inverse problems.</li>
                <li>14:00 - 14:15. Peter Hristov. A cross-platform implementation of BayesCG (live demo).</li>
                <li>14:15 - 14:30. Jon Cockayne. Probabilistic local sensitivity analysis.</li>
                <li>14:30 - 14:45. O. Deniz Akyildiz. Proximal methods from a probabilistic perspective.</li>
                <li>15:30 - 15:45. Filip de Roos. Probabilistic linear algebra (active/adaptive or noisy).</li>
                <li>15:45 - 16:00. Jonathan Wenger. Session Introduction: Ideas for a Probabilistic Numerics Framework.</li>	
            </ul>
        </div>
      </blockquote>

      <h2>Relation between GANs and VAEs via Optimal Transport</h2>

      <blockquote>
        <h3>13th November 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Adversarial Networks and Autoencoders: The Primal-Dual Relationship and Generalization Bounds</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Hisham Husain </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>https://arxiv.org/pdf/1902.00985</li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>20th November 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Kernelized Wasserstein Natural Gradient</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Michael Arbel </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

    </div>
  </section>

  <section class="bs-docs-section">
    <h1 id="contact" class="page-header">Contact</h1>
  
    <h2 id="pages">Email Address</h2>
    <h4> &nbsp; &#8226; &nbsp; tmatsubara@turing.ac.uk </h4>
    <h4> &nbsp; &#8226; &nbsp; j.knoblauch@warwick.ac.uk </h4>
  
    <h2 id="pages">Organizers</h2>
    <h4> (2020 ~) Jeremias Knoblauch & Takuo Matsubara </h4>
    <h4> (2019 ~) Chris Oates & Fran√ßois-Xavier Briol & Marina Riabiz </h4>
  
    <h2 id="pages">Location</h2>
    <h4> The Alan Turing Institute, British Library, 96 Euston Road, London NW1 2DB </h4>
    <h4> https://www.turing.ac.uk </h4>
  
  </section>
