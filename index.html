---
layout: default
title: Home
isHome: true
---

<section class="bs-docs-section">
  <h1 id="overview" class="page-header">Overview</h1>

  <blockquote>
    This is a weakly reading group of Data-Centric Engineering (DCE) group in The Alan Turing Institute (ATI).  
    The reading / talk topics are broad on statistical theory, methodology, and application (e.g. stocastic process, optimization, surrogate modelling, etc).
    Please see our schedule below to check the topics and times for upcoming weeks.  
    This group is open to everyone.  
    There is no requirement to join the talks.  
    If you would like to register for our emailing list for weekly talk reminder or to give your talk in our reading group, please feel free to contact organizers.  
  </blockquote>

</section>


<section class="bs-docs-section">
  <h1 id="schedule" class="page-header">Schedule</h1>

  <h2 id="january">January (Invited Talks + Surrogate Modelling)</h2>
  <blockquote>
    <h3>15th Wednesday 11:00 - 12:00 @ Lovelace Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Monte Carlo wavelets: a randomized approach to frame discretization </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Prof Lorenzo Rosasco (University of Genoa, Italy) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0115" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse in" id="abstract_0115">
        <div class="card card-body abstract-text">
            In this paper we propose and study a family of continuous wavelets on general domains, and a corresponding stochastic discretization that we call Monte Carlo wavelets. 
            First, using tools from the theory of reproducing kernel Hilbert spaces and associated integral operators, we define a family of continuous wavelets by spectral calculus. 
            Then, we propose a stochastic discretization based on Monte Carlo estimates of integral operators. 
            Using concentration of measure results, we establish the convergence of such a discretization and derive convergence rates under natural regularity assumptions.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>22nd Wednesday 11:00 - 12:00 @ Margaret Hamilton Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Bayesian Optimal Design for iterative refocussing </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dr Victoria Volodina (The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0122" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse in" id="abstract_0122">
        <div class="card card-body abstract-text">
            History matching is a type of calibration, that attempts to find input parameters values to achieve the consistency between observations and computer model representation.
            History matching is most effective when it is performed in waves, i.e. refocussing steps (Williamson et al., 2017). 
            At each wave a new ensemble is obtained within the current Not Ruled Out Yet space (NROY), the emulator is updated and the procedure of cutting down the input space is performed again. </br> 
            </br> 
            Generating design for each wave is a challenging problem due to the unusual shapes of NROY space. 
            A number of approaches (Williamson and Vernon, 2013; Gong et al., 2016, Andrianakis et al., 2017) are focused on obtaining space-filling design over the NROY space. 
            In this talk we present a new decision-theoretic method for a design problem for iterative refocussing. 
            We employ a Bayesian experimental design and specify a loss function that compares a volume of NROY space obtained with an updated emulator to the volume of `true' NROY space obtained using a `perfect' emulator.  
            The derived expected loss function contains three independent and interpretable terms. 
            In this talk we compare the effect of proposed Bayesian Optimal Design to space-filling design approaches on the iterative refocussing performed on simulation studies. </br>
            </br> 
            We recognise that adopted Bayesian experimental design involves an expensive optimization problem. 
            Our proposed criterion also could be used to investigate and rank a range of candidate designs for iterative refocussing. 
            In this talk we demonstrate the mathematical justification provided by our Bayesian Design Criterion for each design candidate.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>29th Wednesday 11:00 - 12:00 @ Margaret Hamilton Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Integrated Emulators for Systems of Computer Models </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Mr Deyu Ming (University College London, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0129" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse in" id="abstract_0129">
        <div class="card card-body abstract-text">
            We generalize the state-of-the-art linked emulator for a system of two computer models under the squared exponential kernel to an integrated emulator for any feed-forward system of multiple computer models, under a variety of kernels (exponential, squared exponential, and two key Matérn kernels) that are essential in advanced applications. 
            The integrated emulator combines Gaussian process emulators of individual computer models, and predicts the global output of the system using a Gaussian distribution with explicit mean and variance. 
            By learning the system structure, our integrated emulator outperforms the composite emulator, which emulates the entire system using only global inputs and outputs. 
            Orders of magnitude prediction improvement can be achieved for moderate-size designs. 
            Furthermore, our analytic expressions allow a fast and efficient design algorithm that allocates different runs to individual computer models based on their heterogeneous functional complexity. 
            This design yields either significant computational gains or orders of magnitude reductions in prediction errors for moderate training sizes. We demonstrate the skills and benefits of the integrated emulator in a series of synthetic experiments and a feed-back coupled fire-detection satellite model.
        </div>
    </div>
  </blockquote>

  <h2 id="february">February (Invited Talks + Surrogate Modelling)</h2>
  <blockquote>
    <h3>5th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: A kernel log-rank test of independence</h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dr Tamara Fernadez (University College London, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0205" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse in" id="abstract_0205">
        <div class="card card-body abstract-text">
            With the incorporation of new data gathering methods in clinical research, it becomes fundamental for Survival Analysis techniques to be able to deal with high-dimensional or/and non-standard covariates.
            In this paper we introduce a general non-parametric independence test between right-censored survival times and covariates taking values on a general space $\mathcal{X}$. 
            We show our test statistic has a dual intepretation, first in terms of the supremum of a potentially infinite collection of weight-indexed log-rank tests, with weight functions belonging to a Reproducing kernel Hilbert space of functions (RKHS), and second, as the norm of the difference of the embeddings of certain ``depencency" measures into the RKHS, similarly to the well-know HSIC test-statistic. 
            We provide an easy-to-use test-statistic as well as an economic Wild-Bootstrap procedure and study asymptotic properties of the test, finding sufficient conditions to ensure our test is omnibus. 
            We perform extensive simulations demonstrating that our testing procedure performs, in general, better than competing approaches.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>12nd Wednesday 11:00 - 12:00 @ Lovelace Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Surrogate modelling (TBA)</h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dr James Salter (University of Exeter, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0212" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse" id="abstract_0212">
        <div class="card card-body abstract-text">
            TBA
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>19th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Physical Modelling (TBA) </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dr Monika Kreitmair (University of Cambridge / The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0219" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse" id="abstract_0219">
        <div class="card card-body abstract-text">
            TBA
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>26th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: On the geometry of Stein variational gradient descent</h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dr Andrew Duncan (Imperial College London / The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0226" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse in" id="abstract_0226">
        <div class="card card-body abstract-text">
                Bayesian inference problems require sampling or approximating high-dimensional probability distributions. 
                The focus of this paper is on the recently introduced Stein variational gradient descent methodology, a class of algorithms that rely on iterated steepest descent steps with respect to a reproducing kernel Hilbert space norm. 
                This construction leads to interacting particle systems, the mean-field limit of which is a gradient flow on the space of probability distributions equipped with a certain geometrical structure. 
                We leverage this viewpoint to shed some light on the convergence properties of the algorithm, in particular addressing the problem of choosing a suitable positive definite kernel function. 
                Our analysis leads us to considering certain nondifferentiable kernels with adjusted tails. 
                We demonstrate significant performs gains of these in various numerical experiments.
        </div>
    </div>
  </blockquote>

  <h2 id="march">March</h2>
  <blockquote>
    <h3>6th Friday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: TBA </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dr Helen Ogden (University of Southampton / The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0306" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse" id="abstract_0306">
        <div class="card card-body abstract-text">
            TBA
        </div>
    </div>
  </blockquote>

  <h2 id="april">April</h2>
  <blockquote>
    <h3>Upcoming</h3>
  </blockquote>

  <h2 id="may">May</h2>
  <blockquote>
    <h3>Upcoming</h3>
  </blockquote>

  <h2 id="june">June</h2>
  <blockquote>
    <h3>Upcoming</h3>
  </blockquote>

  <h2 id="july">July</h2>
  <blockquote>
    <h3>Upcoming</h3>
  </blockquote>

  <h2 id="august">August</h2>
  <blockquote>
    <h3>Upcoming</h3>
  </blockquote>

  <h2 id="september">September</h2>
  <blockquote>
    <h3>Upcoming</h3>
  </blockquote>

  <h2 id="october">October</h2>
  <blockquote>
    <h3>Upcoming</h3>
  </blockquote>

  <h2 id="november">November</h2>
  <blockquote>
    <h3>Upcoming</h3>
  </blockquote>

  <h2 id="december">December</h2>
  <blockquote>
    <h3>Upcoming</h3>
  </blockquote>

</section>

<section class="bs-docs-section">
  <h1 id="contact" class="page-header">Contact</h1>

  <h2 id="pages">Email</h2>
  <h4> &nbsp; &#8226; &nbsp; tmatsubara@turing.ac.uk </h4>
  <h4> &nbsp; &#8226; &nbsp; j.knoblauch@warwick.ac.uk </h4>

  <h2 id="pages">Organizers</h2>
  <h4> (2020 ~) Jeremias Knoblauch & Takuo Matsubara </h4>
  <h4> (2019 ~) Chris Oates & François-Xavier Briol & Marina Riabiz </h4>

  <h2 id="pages">Location</h2>
  <h4> The Alan Turing Institute, British Library, 96 Euston Road, London NW1 2DB </h4>
  <h4> https://www.turing.ac.uk </h4>

</section>

<section class="bs-docs-section">
    <h1 id="usage" class="page-header">Past Schedule (2019) <a data-toggle="collapse" href="#past_schedule_2019" role="button" style="color:gray;"> [ click here! ] </a></h1>
    <div class="collapse" id="past_schedule_2019">
      
      <h2>February + March (Optimization)</h2>
      <blockquote>
          <h3>6th February 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Stochastic Gradient Descent </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Marina Riabiz </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
                <li>Leon Bottou, Frank E. Curtis, Jorge Nocedal, <i>Optimization Methods for Large-Scale Machine Learning,</i> <a href="https://arxiv.org/pdf/1606.04838.pdf#page21">https://arxiv.org/pdf/1606.04838.pdf#page21</a></li>
            </ul>
          </div>
      </blockquote>
  
      <blockquote>
          <h3>13th February 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Proof of convergence rate of Stochastic Gradient Descent </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Ömer Deniz Akyıldız </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
          </div>
      </blockquote>
  
      <blockquote>
          <h3>20th February 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Proof of convergence rate of Stochastic Gradient Descent </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Ömer Deniz Akyıldız </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
                <li>Robert M. Gower, <i>Convergence Theorems for Gradient Descent, </i><a href="https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf">https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf</a></li>
                <li>Ji Liu, <i>Stochastic Gradient ‘Descent’ Algorithm</i>, https://www.cs.rochester.edu/u/jliu/CSC-576/class-note-10.pdf <li>
            </ul>
          </div>
      </blockquote>
  
      <blockquote>
          <h3>27th February 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Stochastic Gradient Langevin Dynamics </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Andrew Duncan </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
                <li>Maxim Raginsky, Alexander Rakhlin, Matus Telgarsky, <i>Non-Convex Learning via Stochastic Gradient Langevin Dynamics: A Nonasymptotic Analysis,</i> https://arxiv.org/pdf/1702.03849.pdf </li>
            </ul>
          </div>
      </blockquote>
  
      <blockquote>
          <h3>6th March 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Conjugate Gradient Methods </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Taha Ceriti </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
              <li>Chris Bishop, <i>Neural Networks for Pattern Recognition</i>, Chapter 7. </li>
              <li>J.R. Shewchuk, <i>An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</i>, 1994, https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf </li>
            </ul>
          </div>
      </blockquote>
  
      <blockquote>
          <h3>Extra Reference</h3>
          <div class="card card-body abstract-text">
            <ul type="square">
              <li>S. Bubeck, Convex Optimization: Algorithms and Complexity. In Foundations and Trends in Machine Learning, Vol. 8: No. 3-4, pp 231-357, 2015. <a href="http://sbubeck.com/Bubeck15.pdf" rel="nofollow">http://sbubeck.com/Bubeck15.pdf</a> </li>
              <li>Nagapetyan et al., The True Cost of SGLD, https://arxiv.org/pdf/1706.02692.pdf </li>
              <li>Brosse at al, The promises and pitfalls of Stochastic Gradient Langevin Dynamics, https://arxiv.org/pdf/1811.10072.pdf </li>
              <li>Dalalyan and Karagulyan, User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient, https://arxiv.org/pdf/1710.00095.pdf </li>
              <li>Vollmer et al., Exploration of the (Non-)asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics, https://arxiv.org/pdf/1501.00438.pdf </li>
            </ul>
          </div>
      </blockquote>
  
      <h2>March (Gaussian Processes and RKHS)</h2>
  
      <blockquote>
          <h3>13th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Hyperparameter estimation for Gaussian Processes </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Alex Diaz </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
                <li>Rassmussen, C.E., Gaussian Processes for Machine Learning (Ch2 and Ch 5) <u><a href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf" rel="nofollow">http://www.gaussianprocess.org/gpml/chapters/RW.pdf</a></u></li>
                <li>DiazDelaO, F.A. et al. (2017) Bayesian updating and model class selection with Subset Simulation <u> https://www.sciencedirect.com/science/article/pii/S0045782516308283</u></li>
                <li>Garbuno-Inigo, A. et al. (2016) Gaussian process hyper-parameter estimation using Parallel Asymptotically Independent Markov Sampling <u>https://www.sciencedirect.com/science/article/pii/S0167947316301311</u></li>
                <li>Garbuno-Inigo, A. et al. (2016) Transitional annealed adaptive slice sampling for Gaussian process hyper-parameter estimation <a http://www.dl.begellhouse.com/journals/52034eb04b657aea,55c0c92f02169163,2f3449c01b48b322.html" rel="nofollow">http://www.dl.begellhouse.com/journals/52034eb04b657aea,55c0c92f02169163,2f3449c01b48b322.html</a></li>
            </ul>
          </div>
      </blockquote>

      <blockquote>
        <h3>20th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Gaussian Interpolation/Regression Error Bounds </h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: George Wynne </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>Holger Wendland, Christien Rieger, Approximate Interpolation with Applications to Selecting Smoothing Parameters <u>https://link.springer.com/article/10.1007/s00211-005-0637-y</u></li>
          </ul>
        </div>
      </blockquote>
      
      <blockquote>
        <h3>27th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Structure of the Gaussian RKHS </h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Toni Karvonen </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>Ha Quang Minh, <i>Some Properties of Gaussian Reproducing Kernel Hilbert Spaces and Their Implications for Function Approximation and Learning Theory, </i><u>https://link.springer.com/article/10.1007/s00365-009-9080-0</u></li>
          </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>Extra reference</h3>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>Steinwart, I., Hush, D., & Scovel, C. (2006). An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels. IEEE Transactions on Information Theory, 52(10), 4635–4643</li>
          </ul>
        </div>
      </blockquote>

      <h2>April (Invited Talks + Deep GPs</h2>

      <blockquote>
        <h3>03th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Bayesian synthetic likelihood </h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Leah South </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>L. F. Price, C. C. Drovandi, A. Lee & D. J. Nott, <i>Bayesian Synthetic Likelihood, </i><u>https://www.tandfonline.com/doi/abs/10.1080/10618600.2017.1302882</u> </li>
          </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>12th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Deep Gaussian Processes </h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Kangrui Wang </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>Damianou A, Lawrence N. Deep Gaussian processes <u><a href="http://proceedings.mlr.press/v31/damianou13a.pdf" rel="nofollow">http://proceedings.mlr.press/v31/damianou13a.pdf</a></u></li>
          </ul>
        </div>
      </blockquote>

    </div>
  </section>
