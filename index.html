---
layout: default
title: Home
isHome: true
---

<section class="bs-docs-section">
  <h1 id="overview" class="page-header">Overview</h1>

  <blockquote>
    This is a weekly reading group of Data-Centric Engineering programme in The Alan Turing Institute (<a target="_blank" rel="noopener noreferrer" href="https://www.turing.ac.uk/research/research-programmes/data-centric-engineering" style="color:gray;">https://www.turing.ac.uk/research/research-programmes/data-centric-engineering</a>).  
    The reading / seminar topics are broad on theoretical statistics, methodology, and application e.g. stocastic process, optimization, surrogate modelling, and etc.  
    Please see our schedule below to check upcoming talks.  
    This group is open to everyone and there is no requirement in order to join each talks.  
    If you would like to register for our emailing list or to give a talk on your research, please feel free to contact organizers.  
  </blockquote>

</section>


<section class="bs-docs-section">
  <h1 id="schedule" class="page-header">Schedule</h1>


  <h2 id="january">January (Invited Talks + Surrogate Modelling)</h2>

  <blockquote>
    <h3>15th Wednesday 11:00 - 12:00 @ Lovelace Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Monte Carlo wavelets: a randomized approach to frame discretization </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Lorenzo Rosasco (University of Genoa, Italy) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0115" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse" id="abstract_0115">
        <div class="card card-body abstract-text">
            In this paper we propose and study a family of continuous wavelets on general domains, and a corresponding stochastic discretization that we call Monte Carlo wavelets. 
            First, using tools from the theory of reproducing kernel Hilbert spaces and associated integral operators, we define a family of continuous wavelets by spectral calculus. 
            Then, we propose a stochastic discretization based on Monte Carlo estimates of integral operators. 
            Using concentration of measure results, we establish the convergence of such a discretization and derive convergence rates under natural regularity assumptions.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>22nd Wednesday 11:00 - 12:00 @ Margaret Hamilton Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Bayesian Optimal Design for iterative refocussing </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Victoria Volodina (The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0122" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse" id="abstract_0122">
        <div class="card card-body abstract-text">
            History matching is a type of calibration, that attempts to find input parameters values to achieve the consistency between observations and computer model representation.
            History matching is most effective when it is performed in waves, i.e. refocussing steps (Williamson et al., 2017). 
            At each wave a new ensemble is obtained within the current Not Ruled Out Yet space (NROY), the emulator is updated and the procedure of cutting down the input space is performed again. </br> 
            </br> 
            Generating design for each wave is a challenging problem due to the unusual shapes of NROY space. 
            A number of approaches (Williamson and Vernon, 2013; Gong et al., 2016, Andrianakis et al., 2017) are focused on obtaining space-filling design over the NROY space. 
            In this talk we present a new decision-theoretic method for a design problem for iterative refocussing. 
            We employ a Bayesian experimental design and specify a loss function that compares a volume of NROY space obtained with an updated emulator to the volume of `true' NROY space obtained using a `perfect' emulator.  
            The derived expected loss function contains three independent and interpretable terms. 
            In this talk we compare the effect of proposed Bayesian Optimal Design to space-filling design approaches on the iterative refocussing performed on simulation studies. </br>
            </br> 
            We recognise that adopted Bayesian experimental design involves an expensive optimization problem. 
            Our proposed criterion also could be used to investigate and rank a range of candidate designs for iterative refocussing. 
            In this talk we demonstrate the mathematical justification provided by our Bayesian Design Criterion for each design candidate.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>29th Wednesday 11:00 - 12:00 @ Margaret Hamilton Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Integrated Emulators for Systems of Computer Models </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Deyu Ming (University College London, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0129" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse" id="abstract_0129">
        <div class="card card-body abstract-text">
            We generalize the state-of-the-art linked emulator for a system of two computer models under the squared exponential kernel to an integrated emulator for any feed-forward system of multiple computer models, under a variety of kernels (exponential, squared exponential, and two key Mat√©rn kernels) that are essential in advanced applications. 
            The integrated emulator combines Gaussian process emulators of individual computer models, and predicts the global output of the system using a Gaussian distribution with explicit mean and variance. 
            By learning the system structure, our integrated emulator outperforms the composite emulator, which emulates the entire system using only global inputs and outputs. 
            Orders of magnitude prediction improvement can be achieved for moderate-size designs. 
            Furthermore, our analytic expressions allow a fast and efficient design algorithm that allocates different runs to individual computer models based on their heterogeneous functional complexity. 
            This design yields either significant computational gains or orders of magnitude reductions in prediction errors for moderate training sizes. We demonstrate the skills and benefits of the integrated emulator in a series of synthetic experiments and a feed-back coupled fire-detection satellite model.
        </div>
    </div>
  </blockquote>


  <h2 id="february">February (Invited Talks + Surrogate Modelling)</h2>

  <blockquote>
    <h3>5th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: A kernel log-rank test of independence</h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Tamara Fernadez (University College London, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0205" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse" id="abstract_0205">
        <div class="card card-body abstract-text">
            With the incorporation of new data gathering methods in clinical research, it becomes fundamental for Survival Analysis techniques to be able to deal with high-dimensional or/and non-standard covariates.
            In this paper we introduce a general non-parametric independence test between right-censored survival times and covariates taking values on a general space $\mathcal{X}$. 
            We show our test statistic has a dual intepretation, first in terms of the supremum of a potentially infinite collection of weight-indexed log-rank tests, with weight functions belonging to a Reproducing kernel Hilbert space of functions (RKHS), and second, as the norm of the difference of the embeddings of certain ``depencency" measures into the RKHS, similarly to the well-know HSIC test-statistic. 
            We provide an easy-to-use test-statistic as well as an economic Wild-Bootstrap procedure and study asymptotic properties of the test, finding sufficient conditions to ensure our test is omnibus. 
            We perform extensive simulations demonstrating that our testing procedure performs, in general, better than competing approaches.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>12nd Wednesday 11:00 - 12:00 @ Lovelace Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Efficient high-dimensional emulation and calibration </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: James Salter (University of Exeter / The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0212" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse" id="abstract_0212">
        <div class="card card-body abstract-text">
            Expensive computer models often have high-dimensional spatial and/or temporal outputs, all of which we may be interested in predicting for unseen regions of parameter space, and in comparing to real-world observations (calibration/history matching). 
            When emulating such fields, it is common to either emulate each output individually, or project onto some low dimensional basis (e.g. SVD/PCA, rotations thereof). 
            Typically in a calibration exercise, we will require emulator evaluations for extremely large numbers of points  in the input space, with this problem becoming more computationally intensive as the output size increases. </br>
            </br> 
            We demonstrate several benefits of the basis approach to emulation of such fields, compared to emulating each output individually. 
            In particular, the efficiency that the basis structure allows, both when evaluating predictions at unseen inputs, and in calculating the calibration distance metric (implausibility). 
            We discuss how the basis should be chosen and explore examples from climate and engineering models.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>19nd Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: MultiVerse: Causal Reasoning using Importance Sampling in Probabilistic Programming </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Yura Perov (Babylon Health, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0219" role="button" style="color:gray;"> hide / show </a></h4>
    <div class="collapse" id="abstract_0219">
        <div class="card card-body abstract-text">
            We elaborate on using importance sampling for causal reasoning, in particular for counterfactual inference. 
            We show how this can be implemented natively in probabilistic programming. 
            By considering the structure of the counterfactual query, one can significantly optimise the inference process. 
            We also consider design choices to enable further optimisations. 
            We introduce MultiVerse, a probabilistic programming prototype engine for approximate causal reasoning. 
            We provide experimental results and compare with Pyro, an existing probabilistic programming framework with some of causal reasoning tools.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>26th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: On the geometry of Stein variational gradient descent</h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Andrew Duncan (Imperial College London / The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0226" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse" id="abstract_0226">
        <div class="card card-body abstract-text">
            Bayesian inference problems require sampling or approximating high-dimensional probability distributions. 
            The focus of this paper is on the recently introduced Stein variational gradient descent methodology, a class of algorithms that rely on iterated steepest descent steps with respect to a reproducing kernel Hilbert space norm. 
            This construction leads to interacting particle systems, the mean-field limit of which is a gradient flow on the space of probability distributions equipped with a certain geometrical structure. 
            We leverage this viewpoint to shed some light on the convergence properties of the algorithm, in particular addressing the problem of choosing a suitable positive definite kernel function. 
            Our analysis leads us to considering certain nondifferentiable kernels with adjusted tails. 
            We demonstrate significant performs gains of these in various numerical experiments.
        </div>
    </div>
  </blockquote>


  <h2 id="march">March (Invited Talks + Generalized Bayes)</h2>

  <blockquote>
    <h3>4th Wednesday 13:00 - 14:00 @ Lovelace Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: A kernel two-sample test for functional data </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: George Wynne (Imperial College London, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0304" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse in" id="abstract_0304">
        <div class="card card-body abstract-text">
            Two-sample testing is the task of observing two collections of data samples and determining if they have the same distribution or not. 
            The use of kernel based two-sample tests has seen a lot of success in machine learning, for example in training GANs, due to its helpful computational and theoretical properties. 
            However the vast majority of work done so far in investigating this method has assumed the underlying data are random vectors. 
            We investigate the generalisation of these methods to the case of functional data, where one data point is a function observed at a discrete number of inputs. 
            Such data could still be considered as random vectors but we show how taking the functional view point opens up a large range of theoretical and practical insight. 
            We introduce a new class of kernels which result in consistent tests and discuss their implementation and theoretical properties. 
            This is joint work with Andrew Duncan.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>11th Wednesday 13:00 - 14:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: General Bayesian Updating </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Edwin Fong (University of Oxford / The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0311" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse in" id="abstract_0311">
        <div class="card card-body abstract-text">
            General Bayesian updating is a coherent framework for updating a prior belief distribution to a posterior, where the beliefs are over a parameter of interest that is related to observed data through a loss function, instead of directly corresponding to a likelihood function. 
            This is allows us to bypass the Bayesian machinery of modelling the true data-generating distribution which is becoming increasingly cumbersome as datasets grow in size and complexity. 
            The belief update is derived through decision theoretic arguments, and in particular requires coherence of beliefs. 
            I will be discussing the foundations of general Bayesian updating, as well as some more recent work on selecting the tempering parameter, links below. </br>
            </br>
            <b>Links:</b></br>
            <a target="_blank" rel="noopener noreferrer" href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12158">https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12158</a></br>
            <a target="_blank" rel="noopener noreferrer" href="https://academic.oup.com/biomet/article/106/2/465/5385582">https://academic.oup.com/biomet/article/106/2/465/5385582</a>
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>25th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Nonparametric Learning with the Posterior Bootstrap </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Edwin Fong (University of Oxford / The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0325" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse in" id="abstract_0325">
        <div class="card card-body abstract-text">
            We present a scalable Bayesian nonparametric learning routine that enables posterior sampling through the optimization of suitably randomized objective functions. 
            A Dirichlet process prior on the unknown data distribution accounts for model misspecification and admits an embarrassingly parallel posterior bootstrap algorithm that generates independent and exact samples from the nonparametric posterior distribution. 
            Our method has attractive theoretical properties and is particularly adept at sampling from multimodal posterior distributions via a random restart mechanism. 
            I will be presenting our paper as well as a summary of previous work, links below. </br>
            </br>
            <b>Links:</b></br>
            <a target="_blank" rel="noopener noreferrer" href="http://proceedings.mlr.press/v97/fong19a/fong19a.pdf">http://proceedings.mlr.press/v97/fong19a/fong19a.pdf</a></br>
            <a target="_blank" rel="noopener noreferrer" href="https://papers.nips.cc/paper/7477-nonparametric-learning-from-bayesian-models-with-randomized-objective-functions.pdf">https://papers.nips.cc/paper/7477-nonparametric-learning-from-bayesian-models-with-randomized-objective-functions.pdf</a>
        </div>
    </div>
  </blockquote>


  <h2 id="april">April (Invited Talks + Generalized Bayes)</h2>

  <blockquote>
    <h3>1st Wednesday 11:00 - 12:00 @ Lovelace Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Augmenting Statistics with a representation of deterministic uncertainty </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Jeremie Houssineau (University of Warwick, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0401" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse in" id="abstract_0401">
        <div class="card card-body abstract-text">
            It is commonly accepted in statistics that the only way of dealing with uncertainty is through the mathematical tools characterising random phenomena. 
            However, it is often information, or rather the lack of it, that is the source of uncertainty. 
            Using probability theory in this case has known limitations such as the absence of non-informative priors in unbounded spaces. 
            The objective of this talk is to show how the measure-theoretic concept of outer measure can be used to model the lack of information about some parameters in a statistical model and to derive practical algorithms that complement the existing wealth of statistical techniques in a principled and yet intuitive way.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>6th Wednesday 13:00 - 14:00 @ Lovelace Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Optimistic bounds for multi-output prediction </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Henry Reeve (University of Birmingham, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0406" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse in" id="abstract_0406">
        <div class="card card-body abstract-text">
            We investigate the challenge of multi-output learning, where the goal is to learn a vector-valued function based on a supervised data set. 
            This includes a range of important problems in Machine Learning including multi-target regression, multi-class classification and multi-label classification. 
            We begin our analysis by introducing the self-bounding Lipschitz condition for multi-output loss functions, which interpolates continuously between a classical Lipschitz condition and a multi-dimensional analogue of a smoothness condition. 
            We then show that the self-bounding Lipschitz condition gives rise to optimistic bounds for multi-output learning, which are minimax optimal up to logarithmic factors. 
            The proof exploits local Rademacher complexity combined with a powerful minoration inequality due to Srebro, Sridharan and Tewari. 
            As an application we derive a state-of-the-art generalization bound for multi- class gradient boosting.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>8th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: On the stability of general Bayesian inference </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Jack Jewson (University of Warwick, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0408" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse in" id="abstract_0408">
        <div class="card card-body abstract-text">
            Bayesian stability and sensitivity analyses have traditionally investigated the stability of posterior inferences to the subjective specification of the prior. 
            We extend this to consider the stability of inference to the selection of the likelihood function. 
            Rather than making the unrealistic assumption that the model is correctly specified for the process that generated the data, we assume that it is at best one of many possible approximations of the underlying process broadly representing the decision maker's (DM) beliefs. 
            We thus investigate the stability of posterior inferences to the arbitrary selection of one of these approximations. 
            We show that traditional Bayesian updating, minimising the Kullback-Leibler Divergence (KLD) between the sample distribution of the observations and the model, is only stable to a very strict class of likelihood models. 
            On the other hand, generalised Bayesian inference (Bissiri et al., 2016) minimising the beta-divergence (betaD) is shown to be stable across interpretable neighbourhoods of likelihood models. 
            The result is that using Bayes' rule requires a DM to be unrealistically sure about the probability judgements made by their model while updating using the betaD is stable to reasonable perturbations from the chosen model. 
            We illustrate this for several regression and classification examples including a Bayesian on-line changepoint detection algorithm applied to air pollution data from the City of London.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>15th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Generalized Bayesian Inference </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Jeremias Knoblauch </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0415" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse" id="abstract_0415">
        <div class="card card-body abstract-text">
            TBA
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>22nd Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: Generalized Bayesian Filtering via Sequential Monte Carlo </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Ayman Boustati </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0422" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse in" id="abstract_0422">
        <div class="card card-body abstract-text">
            We introduce a framework for inference in general state-space hidden Markov models (HMMs) under likelihood misspecification. 
            In particular, we leverage the loss-theoretic perspective of generalized Bayesian inference (GBI) to define generalized filtering recursions in HMMs, that can tackle the problem of inference under model misspecification. 
            In doing so, we arrive at principled procedures for robust inference against observation contamination through the Œ≤-divergence. 
            Operationalizing the proposed framework is made possible via sequential Monte Carlo methods (SMC). The standard particle methods, and their associated convergence results, are readily generalized to the new setting. 
            We demonstrate our approach to object tracking and Gaussian process regression problems, and observe improved performance over standard filtering algorithms.
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>29th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: TBA </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Omar Rivasplata (University College London / DeepMind, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0429" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse" id="abstract_0429">
        <div class="card card-body abstract-text">
            TBA
        </div>
    </div>
  </blockquote>


  <h2 id="may">May (Invited Talks + Stochastic Optimization)</h2>

  <blockquote>
    <h3>6th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: TBA </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: TBA </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0506" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse" id="abstract_0506">
        <div class="card card-body abstract-text">
            TBA
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>13th Wednesday 13:00 - 14:00 @ Lovelace Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: TBA </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Dennis Prangle (Newcastle University, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0513" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse" id="abstract_0513">
        <div class="card card-body abstract-text">
            TBA
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>20th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: TBA </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: Omer Deniz Akyildiz (University of Warwick / The Alan Turing Institute, UK) </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0520" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse" id="abstract_0520">
        <div class="card card-body abstract-text">
            TBA
        </div>
    </div>
  </blockquote>

  <blockquote>
    <h3>27th Wednesday 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
    <h4>&nbsp; &#8226; &nbsp; Topic: TBA </h4>
    <h4>&nbsp; &#8226; &nbsp; Speaker: TBA </h4>
    <h4>&nbsp; &#8226; &nbsp; Abstract: <a data-toggle="collapse" href="#abstract_0527" role="button" style="color:gray;"> hide / show </a> </h4>
    <div class="collapse" id="abstract_0527">
        <div class="card card-body abstract-text">
            TBA
        </div>
    </div>
  </blockquote>


  <h2 id="may-december">June ~ December (under arrangement)</h2>

  <blockquote>
    <h3>Upcoming more!</h3>
  </blockquote>

</section>

<section class="bs-docs-section">
    <h1 id="past_schedule_2019" class="page-header">Past Schedule (2019) <a data-toggle="collapse" href="#div_past_schedule_2019" role="button" style="color:gray;"> [click here!] </a></h1>
    <div class="collapse" id="div_past_schedule_2019">
      
      <h2>Optimization</h2>
      <blockquote>
          <h3>6th February 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Stochastic Gradient Descent </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Marina Riabiz </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
                <li>Leon Bottou, Frank E. Curtis, Jorge Nocedal, <i>Optimization Methods for Large-Scale Machine Learning,</i> <a href="https://arxiv.org/pdf/1606.04838.pdf#page21">https://arxiv.org/pdf/1606.04838.pdf#page21</a></li>
            </ul>
          </div>
      </blockquote>
  
      <blockquote>
          <h3>13th February 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Proof of convergence rate of Stochastic Gradient Descent </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: √ñmer Deniz Akyƒ±ldƒ±z </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
          </div>
      </blockquote>
  
      <blockquote>
          <h3>20th February 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Proof of convergence rate of Stochastic Gradient Descent </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: √ñmer Deniz Akyƒ±ldƒ±z </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
                <li>Robert M. Gower, <i>Convergence Theorems for Gradient Descent, </i><a href="https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf">https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf</a></li>
                <li>Ji Liu, <i>Stochastic Gradient ‚ÄòDescent‚Äô Algorithm</i>, https://www.cs.rochester.edu/u/jliu/CSC-576/class-note-10.pdf </li>
            </ul>
          </div>
      </blockquote>
  
      <blockquote>
          <h3>27th February 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Stochastic Gradient Langevin Dynamics </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Andrew Duncan </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
                <li>Maxim Raginsky, Alexander Rakhlin, Matus Telgarsky, <i>Non-Convex Learning via Stochastic Gradient Langevin Dynamics: A Nonasymptotic Analysis,</i> https://arxiv.org/pdf/1702.03849.pdf </li>
            </ul>
          </div>
      </blockquote>
  
      <blockquote>
          <h3>6th March 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Conjugate Gradient Methods </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Taha Ceriti </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
              <li>Chris Bishop, <i>Neural Networks for Pattern Recognition</i>, Chapter 7. </li>
              <li>J.R. Shewchuk, <i>An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</i>, 1994, https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf </li>
            </ul>
          </div>
      </blockquote>
  
      <blockquote>
          <h3>(Extra Reference)</h3>
          <div class="card card-body abstract-text">
            <ul type="square">
              <li>S. Bubeck, Convex Optimization: Algorithms and Complexity. In Foundations and Trends in Machine Learning, Vol. 8: No. 3-4, pp 231-357, 2015. <a href="http://sbubeck.com/Bubeck15.pdf" rel="nofollow">http://sbubeck.com/Bubeck15.pdf</a> </li>
              <li>Nagapetyan et al., The True Cost of SGLD, https://arxiv.org/pdf/1706.02692.pdf </li>
              <li>Brosse at al, The promises and pitfalls of Stochastic Gradient Langevin Dynamics, https://arxiv.org/pdf/1811.10072.pdf </li>
              <li>Dalalyan and Karagulyan, User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient, https://arxiv.org/pdf/1710.00095.pdf </li>
              <li>Vollmer et al., Exploration of the (Non-)asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics, https://arxiv.org/pdf/1501.00438.pdf </li>
            </ul>
          </div>
      </blockquote>
  
      <h2>Gaussian Processes and RKHS</h2>
  
      <blockquote>
          <h3>13th March 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
          <h4>&nbsp; &#8226; &nbsp; Topic: Hyperparameter estimation for Gaussian Processes </h4>
          <h4>&nbsp; &#8226; &nbsp; Speaker: Alex Diaz </h4>
          <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
          <div class="card card-body abstract-text">
            <ul type="square">
                <li>Rassmussen, C.E., Gaussian Processes for Machine Learning (Ch2 and Ch 5) <u><a href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf" rel="nofollow">http://www.gaussianprocess.org/gpml/chapters/RW.pdf</a></u></li>
                <li>DiazDelaO, F.A. et al. (2017) Bayesian updating and model class selection with Subset Simulation <u> https://www.sciencedirect.com/science/article/pii/S0045782516308283</u></li>
                <li>Garbuno-Inigo, A. et al. (2016) Gaussian process hyper-parameter estimation using Parallel Asymptotically Independent Markov Sampling <u>https://www.sciencedirect.com/science/article/pii/S0167947316301311</u></li>
                <li>Garbuno-Inigo, A. et al. (2016) Transitional annealed adaptive slice sampling for Gaussian process hyper-parameter estimation <a http://www.dl.begellhouse.com/journals/52034eb04b657aea,55c0c92f02169163,2f3449c01b48b322.html" rel="nofollow">http://www.dl.begellhouse.com/journals/52034eb04b657aea,55c0c92f02169163,2f3449c01b48b322.html</a></li>
            </ul>
          </div>
      </blockquote>

      <blockquote>
        <h3>20th March 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Gaussian Interpolation/Regression Error Bounds </h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: George Wynne </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>Holger Wendland, Christien Rieger, Approximate Interpolation with Applications to Selecting Smoothing Parameters <u>https://link.springer.com/article/10.1007/s00211-005-0637-y</u></li>
          </ul>
        </div>
      </blockquote>
      
      <blockquote>
        <h3>27th March 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Structure of the Gaussian RKHS </h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Toni Karvonen </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>Ha Quang Minh, <i>Some Properties of Gaussian Reproducing Kernel Hilbert Spaces and Their Implications for Function Approximation and Learning Theory, </i><u>https://link.springer.com/article/10.1007/s00365-009-9080-0</u></li>
          </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>(Extra reference)</h3>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>Steinwart, I., Hush, D., & Scovel, C. (2006). An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels. IEEE Transactions on Information Theory, 52(10), 4635‚Äì4643</li>
          </ul>
        </div>
      </blockquote>

      <h2>Invited Talks + Deep GPs</h2>

      <blockquote>
        <h3>03th April 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Bayesian synthetic likelihood </h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Leah South </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>L. F. Price, C. C. Drovandi, A. Lee & D. J. Nott, <i>Bayesian Synthetic Likelihood, </i><u>https://www.tandfonline.com/doi/abs/10.1080/10618600.2017.1302882</u> </li>
          </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>12th April 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Deep Gaussian Processes </h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Kangrui Wang </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li>Damianou A, Lawrence N. Deep Gaussian processes <u><a href="http://proceedings.mlr.press/v31/damianou13a.pdf" rel="nofollow">http://proceedings.mlr.press/v31/damianou13a.pdf</a></u></li>
            <li>Dunlop M M, Girolami M A, Stuart A M, et al. How deep are deep Gaussian processes? <u><a href="http://www.jmlr.org/papers/volume19/18-015/18-015.pdf" rel="nofollow">http://www.jmlr.org/papers/volume19/18-015/18-015.pdf</a></u></li>
            <li>Bauer M, van der Wilk M, Rasmussen C E. Understanding probabilistic sparse Gaussian process approximations</i><u>https://arxiv.org/abs/1606.04820</u> </li>
          </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>17th April 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Multi Level Monte Carlo</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Alastair Gregory </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
          <ul type="square">
            <li><a href="http://people.maths.ox.ac.uk/gilesm/files/cgst.pdf" rel="nofollow">http://people.maths.ox.ac.uk/gilesm/files/cgst.pdf</a></li>
            <li>https://people.maths.ox.ac.uk/gilesm/files/OPRE_2008.pdf</li>
          </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>24th April 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Adaptive Bayesian Quadrature</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Matthew Fisher </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <h2>MMD & Stein's Method</h2>

      <blockquote>
        <h3>01st May 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Controlling Convergence with Maximum Mean Discrepancy</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Chris Oates </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Gretton, A., Borgwardt, K., Rasch, M., Sch√∂lkopf, B. and Smola, A.J., 2007. <i>A kernel method for the two-sample-problem</i>. <u><a href="http://papers.nips.cc/paper/3110-a-kernel-method-for-the-two-sample-problem.pdf" rel="nofollow">http://papers.nips.cc/paper/3110-a-kernel-method-for-the-two-sample-problem.pdf</a></u> </li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>08th May 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Introduction to Stein‚Äôs method</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: FX Briol </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Talk will cover Chapter 2 of ‚ÄúChen, L. H. Y., Goldstein, L., & Shao, Q.-M. (2011). Normal Approximation by Stein‚Äôs Method. Springer.‚Äù</li>
                <li>Gorham,  J., Duncan, A., Mackey, L., & Vollmer, S. (2016). Measuring Sample Quality with Diffusions. ArXiv:1506.03039.</li>
                <li>Gorham, J., & Mackey, L. (2017). Measuring Sample Quality with Kernels. In Proceedings of the International Conference on Machine Learning (pp. 1292‚Äì1301).</li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>15th May 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Bochner's Theorem and Maximum Mean Discrepancy</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: George Wynne </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Theorem 9 in <u><a href="http://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf" rel="nofollow">http://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf</a></u> is the result of Bochner's theorem being used in MMD</li>
                <li><u><a href="http://www.math.nus.edu.sg/~matsr/ProbI/Lecture7.pdf" rel="nofollow">http://www.math.nus.edu.sg/~matsr/ProbI/Lecture7.pdf</a></u><b> </b>is a source for proof of Bochner's theorem</li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>(Extra Reference)</h3>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>https://sites.google.com/site/steinsmethod/home</li>
            </ul>
        </div>
      </blockquote>

      <h2>Greedy Algorithms & Uncertainty Exploration in Aerospace</h2>

      <blockquote>
        <h3>19th June 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Convergence Guarantees for Adaptive Bayesian Quadrature Methods</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Motonobu Kanagawa </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Kanagawa, M. and Hennig, P, 2019. <i>Convergence Guarantees for Adaptive Bayesian Quadrature Methods</i>. <u>https://arxiv.org/abs/1905.10271</u> </li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>26th June 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Using machine learning to predict and understand turbulence modelling uncertainties</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Ashley Scillitoe </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <blockquote>
        <h3>03rd July 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Polynomial approximations in uncertainty quantification</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Pranay Seshadri </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Talk will cover Xiu D (2010) ‚ÄúNumerical methods for stochastic computations: a spectral method approach‚Äù. Princeton University Press.</li>
            </ul>
        </div>
      </blockquote>

      <h2>Bayesian Learning and Algorithms</h2>

      <blockquote>
        <h3>10th July 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: A Kernel Stein Test for Comparing Latent Variable Models</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Heishiro Kanagawa </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Kanagawa, H., Jitkrittum, W., Mackey, L., Fukumizu, K., Gretton, A. (2019) <i>A Kernel Stein Test for Comparing Latent Variable Models. </i> <u>https://arxiv.org/abs/1907.00586</u> </li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>17th July 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Multi-resolution Multi-task Gaussian Processes</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Oliver Hamelijnck </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Hamelijnck, O., Damoulas, T., Wang, K., Girolami, M. (2019) <i>Multi-resolution Multi-task Gaussian Processes. </i> <u>https://arxiv.org/pdf/1906.08344.pdf</u> </li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>24th July 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: A Primer on PAC Bayesian Learning</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Benjamin Guedj </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Talk will cover <a href="https://bguedj.github.io/icml2019/index.html">this tutorial from ICML 2019</a></li>
            </ul>
        </div>
      </blockquote>

      <h2>Gradient Flows</h2>

      <blockquote>
        <h3>31th July 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: An Introduction to Measure Transport</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Chris Oates </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <blockquote>
        <h3>06th August 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Gradient Flows for Statistical Computation</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Marina Riabiz </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li> Daneri and Savare', Lecture Notes on Gradient Flows and Optimal Transport, <u>https://arxiv.org/abs/1009.3737</u></li>
                <li> N. Garcia Trillos, D. Sanz-Alonso, The Bayesian update: variational formulations and gradient flows, <u>https://arxiv.org/abs/1705.07382</u></li>
                <li> G.Peyre' and M. Cuturi, Computational Optimal Transport, Chapter 9.3 <u>https://arxiv.org/pdf/1803.00567.pdf</u></li>
                <li> Carrillo, Craig, Patacchini, A blob method for diffusion, <u>https://arxiv.org/abs/1709.09195</u></li>
                <li> Sides <u>  http://web.math.ucsb.edu/~kcraig/math/curriculum_vitae_files/NIPS_120917.pdf</u></li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>14th August 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: The Mathematics of Gradient Flows</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Andrew Duncan </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <blockquote>
        <h3>14th August 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Displacement Convexity and Implications for Variational Inference</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Andrew Duncan </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <h2>Estimators for Intractable Models</h2>

      <blockquote>
        <h3>23th August 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Comparing spatial models in the presence of spatial smoothing</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Earl Duncan</h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <blockquote>
        <h3>28th August 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Fisher efficient inference of intractable models</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Song Liu </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Liu, S., Kanamori, T., Jitkrittum, W., & Chen, Y. (2018). <i>Fisher efficient inference of intractable models. </i> <u>https://arxiv.org/abs/1805.07454</u> </li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>04th September 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Statistical Inference for Generative Models with Maximum Mean Discrepancy</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: FX Briol </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Briol, F-X, Barp, A., Duncan, A. B., Girolami, M. (2019) <i>Statistical inference for generative models with maximum mean discrepancy. </i> <u>https://arxiv.org/abs/1906.05944</u> </li>
            </ul>
        </div>
      </blockquote>

      <h2>Uncertainty Quantification and Probabilistic Numerics</h2>

      <blockquote>
        <h3>11th September 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: A New Approach to Probabilistic Rounding Error Analysis</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Jon Cockayne </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Talk will cover Higham, N., Mary, T. (2018) <i>A new approach to probabilistic rounding analysis. </i> <u>http://eprints.maths.manchester.ac.uk/2673/1/paper.pdf</u> </li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>25th September 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Model Inference for Ordinary Differential Equations by Parametric Polynomial Kernel Regression</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: David Green </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <blockquote>
        <h3>02nd October 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: The Ridgelet Transform and a Quadrature of Neural Networks</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Takuo Matsubara </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <blockquote>
        <h3>09th October 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Hierarchical and multivariate Gaussian processes for environmental and ecological applications</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Jarno Vanhatalo </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <h2>PAC Bayes</h2>

      <blockquote>
        <h3>16th October 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: TBA</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Omar Rivasplata </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

      <h2>Inference for Stochastic Processes</h2>

      <blockquote>
        <h3>23th October 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Learning Laws of Stochastic Processes</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Harald Oberhauser </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>Chevyrev, I., & Oberhauser, H. (2018). Signature moments to characterize laws of stochastic processes. arXiv:1810.10971.</li>
            </ul>
        </div>
      </blockquote>

      <h2>Probabilistic Numerics 2019 Workshop</h2>

      <blockquote>
        <h3>29th October 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>10:00 - 10:15. Chris Oates. Welcome, and Some Open Problems in Probabilistic Numerics.</li>
                <li>10:15 - 10:30. Francois-Xavier Briol. Probabilistic Numerics via Transfer Learning.</li>
                <li>10:30 - 10:45. Toni Karvonen. Uncertainty quantification with Gaussian processes and when to trust a PN method.</li>
                <li>10:45 - 11:00. Simo S√§rkk√§. Numerical Integration as a Finite Matrix Approximation to Multiplication Operator.</li>
                <li>11:30 - 11:45. Motonobu Kanagawa. Open questions regarding adaptive quadrature methods.</li>
                <li>11:45 - 12:00. George Wynne. Gaussian process error bounds from a sampling inequality.</li>
                <li>14:00 - 14:15. Peter Hristov. Surrogate modelling with probabilistic numerics.</li>
                <li>14:15 - 14:30. Filip Tronarp. On Gaussian Filtering/Smoothing for Solving ODEs.</li>
                <li>14:30 - 14:45. Takuo Matsubara. Bayesian quadrature of neural networks based on the Ridgelet transform.</li>
                <li>15:30 - 15:45. Alex Diaz. PN for eigenvalue problems.</li>
                <li>15:45 - 16:00. Maren Mahsereci. Software for PN.</li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>30th October 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>10:00 - 10:15. Alex Gessner. Acquisition functions for adaptive Bayesian quadrature.</li>
                <li>10:15 - 10:30. Mark Girolami. Title TBC.</li>
                <li>10:30 - 10:45. Takeru Matsuda. ODE parameter estimation with discretization error quantification.</li>
                <li>11:30 - 11:45. Matthew Fisher. Locally Adaptive Bayesian Cubature.</li>
                <li>11:45 - 12:00. Daniel Tait. Constrained VI for inverse problems.</li>
                <li>14:00 - 14:15. Peter Hristov. A cross-platform implementation of BayesCG (live demo).</li>
                <li>14:15 - 14:30. Jon Cockayne. Probabilistic local sensitivity analysis.</li>
                <li>14:30 - 14:45. O. Deniz Akyildiz. Proximal methods from a probabilistic perspective.</li>
                <li>15:30 - 15:45. Filip de Roos. Probabilistic linear algebra (active/adaptive or noisy).</li>
                <li>15:45 - 16:00. Jonathan Wenger. Session Introduction: Ideas for a Probabilistic Numerics Framework.</li>	
            </ul>
        </div>
      </blockquote>

      <h2>Relation between GANs and VAEs via Optimal Transport</h2>

      <blockquote>
        <h3>13th November 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Adversarial Networks and Autoencoders: The Primal-Dual Relationship and Generalization Bounds</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Hisham Husain </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
            <ul type="square">
                <li>https://arxiv.org/pdf/1902.00985</li>
            </ul>
        </div>
      </blockquote>

      <blockquote>
        <h3>20th November 11:00 - 12:00 @ Mary Shelley Room at ATI</h3>
        <h4>&nbsp; &#8226; &nbsp; Topic: Kernelized Wasserstein Natural Gradient</h4>
        <h4>&nbsp; &#8226; &nbsp; Speaker: Michael Arbel </h4>
        <h4>&nbsp; &#8226; &nbsp; Reference: </h4>
        <div class="card card-body abstract-text">
        </div>
      </blockquote>

    </div>
  </section>

  <section class="bs-docs-section">
    <h1 id="contact" class="page-header">Contact</h1>
  
    <h2 id="pages">Email Address</h2>
    <h4> &nbsp; &#8226; &nbsp; tmatsubara@turing.ac.uk </h4>
    <h4> &nbsp; &#8226; &nbsp; j.knoblauch@warwick.ac.uk </h4>
  
    <h2 id="pages">Organizers</h2>
    <h4> (2020 ~) Jeremias Knoblauch & Takuo Matsubara </h4>
    <h4> (2019 ~) Chris Oates & Fran√ßois-Xavier Briol & Marina Riabiz </h4>
  
    <h2 id="pages">Location</h2>
    <h4> The Alan Turing Institute, British Library, 96 Euston Road, London NW1 2DB </h4>
    <h4> https://www.turing.ac.uk </h4>
  
  </section>
